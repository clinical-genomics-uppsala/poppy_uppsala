__author__ = "Arielle R. Munters"
__copyright__ = "Copyright 2024, Arielle R. Munters"
__email__ = "arielle.munters@scilifelab.uu.se"
__license__ = "GPL-3"

import os.path
import requests


ruleorder: filtering_bcftools_filter_include_region > snv_indels_bgzip


# Include pipeline specific rules
include: "rules/common.smk"
include: "rules/version.smk"
include: "rules/bamsnap.smk"
include: "rules/results_report.smk"
include: "rules/sample_order_multiqc.smk"


# enforces version update first
ruleorder: version_update_poppy > bamsnap_create_pos_list
ruleorder: copy_alignment_bam_file_index > alignment_samtools_index


# 'All' rule, must be specified before any other modules are
# included, since they also contain 'All' rule
rule all:
    input:
        compile_output_file_list,


# Include modules
module alignment:
    snakefile:
        get_module_snakefile(
            config,
            "hydra-genetics/alignment",
            path="workflow/Snakefile",
            tag=config["modules"]["alignment"],
        )
    config:
        config


use rule samtools_index from alignment as alignment_samtools_index with:
    output:
        bai="{file}.bam.bai",
    wildcard_constraints:
        file="^alignment/.+",


try:  # if on a system that is connected to the internet
    response = requests.get("https://github.com/genomic-medicine-sweden/poppy.git", timeout=3)
    module poppy:
        snakefile:
            github(
                "genomic-medicine-sweden/poppy",
                path="workflow/Snakefile",
                tag=config["poppy_version"],
            )
        config:
            config

except OSError as e:
    module poppy:
        snakefile:
            # os.path.join(os.path.dirname(os.getcwd()), "poppy", "workflow", "Snakefile")
            os.path.join(config["POPPY_HOME"], "workflow", "Snakefile")
        config:
            config


use rule * from poppy exclude all, copy_bamsnap


# module alignment: ...


# use rule picard_mark_duplicates from alignment as alignment_picard_rm_duplicates
# input
# output
# params: extra = "--REMOVE_DUPLICATES true"

module snv_indels:
    snakefile:
        get_module_snakefile(
                config,
                "hydra-genetics/snv_indels",
                path="workflow/Snakefile",
                tag=config["modules"]["snv_indels_uppsala"],
            )
    config:
        config


use rule gatk_mutect2 from snv_indels as snv_indels_gatk_mutect2 with:
    output:
        bam="snv_indels/gatk_mutect2/{sample}_{type}_{chr}.unfiltered.bam",
        bai="snv_indels/gatk_mutect2/{sample}_{type}_{chr}.unfiltered.bai",
        stats=temp("snv_indels/gatk_mutect2/{sample}_{type}_{chr}.unfiltered.vcf.gz.stats"),
        vcf=temp("snv_indels/gatk_mutect2/{sample}_{type}_{chr}.unfiltered.vcf.gz"),
        tbi=temp("snv_indels/gatk_mutect2/{sample}_{type}_{chr}.unfiltered.vcf.gz.tbi"),
        f1f2=temp("snv_indels/gatk_mutect2/{sample}_{type}_{chr}.unfiltered.f1r2.tar.gz"),


use rule merge_af_complex_variants from snv_indels as snv_indels_merge_af_complex_variants_unsorted with:
    input:
        vcf="snv_indels/{caller}/{sample}_{type}.normalized.vcf.gz",
    output:
        vcf=temp("snv_indels/{caller}/{sample}_{type}.normalized.merged_af.unsorted.vcf.gz"),
    params:
        merge_method=config.get("merge_af_complex_variants", {}).get("merge_method", "skip"),
    log:
        "snv_indels/{caller}/{sample}_{type}.normalized.merged_af.vcf.gz.log",
    benchmark:
        repeat(
            "snv_indels/{caller}/{sample}_{type}.normalized.merged_af.vcf.gz.benchmark.tsv",
            config.get("merge_af_complex_variants", {}).get("benchmark_repeats", 1),
        )


use rule bcftools_sort from snv_indels as snv_indels_bcftools_sort_merged_af with:
    input:
        vcf="snv_indels/{caller}/{sample}_{type}.normalized.merged_af.unsorted.vcf.gz",
    output:
        vcf=temp("snv_indels/{caller}/{sample}_{type}.normalized.merged_af.sorted.vcf.gz"),
    log:
        "snv_indels/{caller}/{sample}_{type}.normalized.merged_af.sorted.vcf.gz.log",
    benchmark:
        repeat(
            "snv_indels/{caller}/{sample}_{type}.normalized.merged_af.sorted.vcf.gz.benchmark.tsv",
            config.get("bcftools_sort", {}).get("benchmark_repeats", 1),
        )


use rule merge_af_complex_variants from snv_indels as snv_indels_merge_af_complex_variants_pindel with:
    input:
        vcf="cnv_sv/pindel_vcf/{sample}_{type}.no_tc.normalized.sorted.vcf.gz",
        tabix="cnv_sv/pindel_vcf/{sample}_{type}.no_tc.normalized.sorted.vcf.gz.tbi",
    output:
        vcf=temp("cnv_sv/pindel_vcf/{sample}_{type}.no_tc.normalized.merged_af.vcf.gz"),
    params:
        merge_method=config.get("merge_af_complex_variants", {}).get("merge_method", "skip"),
    log:
        "cnv_sv/pindel_vcf/{sample}_{type}.no_tc.normalized.merged_af.vcf.gz.log",
    benchmark:
        repeat(
            "cnv_sv/pindel_vcf/{sample}_{type}.no_tc.normalized.merged_af.vcf.gz.benchmark.tsv",
            config.get("merge_af_complex_variants", {}).get("benchmark_repeats", 1),
        )


use rule bcbio_variation_recall_ensemble from snv_indels as snv_indels_bcbio_variation_recall_ensemble with:
    input:
        vcfs=expand(
            "snv_indels/{caller}/{{sample}}_{{type}}.normalized.merged_af.sorted.vcf.gz",
            caller=config.get("bcbio_variation_recall_ensemble", {}).get("callers", []),
        ),
        ref=config["reference"]["fasta"],
    output:
        vcf=temp("snv_indels/bcbio_variation_recall_ensemble/{sample}_{type}.ensembled.vcf.gz"),
        bcbio_work=temp(directory("snv_indels/bcbio_variation_recall_ensemble/{sample}_{type}.ensembled-work/")),
    log:
        "snv_indels/bcbio_variation_recall_ensemble/{sample}_{type}.ensembled.vcf.gz.log",
    benchmark:
        repeat(
            "snv_indels/bcbio_variation_recall_ensemble/{sample}_{type}.ensembled.vcf.gz.benchmark.tsv",
            config.get("bcbio_variation_recall_ensemble", {}).get("benchmark_repeats", 1),
        )


use rule bcftools_concat from snv_indels as snv_indels_concat_vcfs with:
    input:
        calls=[
            "snv_indels/bcbio_variation_recall_ensemble/{sample}_{type}.ensembled.vep_annotated.artifact_annotated.background_annotated.filter.somatic.vcf.gz",
            "cnv_sv/pindel_vcf/{sample}_{type}.no_tc.normalized.vep_annotated.artifact_annotated.vcf.gz",
        ],
        index=[
            "snv_indels/bcbio_variation_recall_ensemble/{sample}_{type}.ensembled.vep_annotated.artifact_annotated.background_annotated.filter.somatic.vcf.gz.tbi",
            "cnv_sv/pindel_vcf/{sample}_{type}.no_tc.normalized.vep_annotated.artifact_annotated.vcf.gz.tbi",
        ],
    output:
        vcf=temp("snv_indels/bcftools_concat/{sample}_{type}.filter.somatic.vcf.gz"),
    params:
        extra=config.get("bcftools_concat_vcfs", {}).get("extra", ""),
    log:
        "snv_indels/bcftools_concat/{sample}_{type}.filter.somatic.vcf.gz.log",
    benchmark:
        repeat(
            "snv_indels/bcftools_concat/{sample}_{type}.filter.somatic.vcf.gz.benchmark.tsv",
            config.get("bcftools_concat", {}).get("benchmark_repeats", 1),
        )


module qc:
    snakefile:
        get_module_snakefile(
            config,
            "hydra-genetics/qc",
            path="workflow/Snakefile",
            tag=config["modules"]["qc"],
        )
    config:
        config


use rule mosdepth_bed from qc as qc_mosdepth_bed_coding with:
    input:
        bam="alignment/samtools_merge_bam/{sample}_{type}.bam",
        bai="alignment/samtools_merge_bam/{sample}_{type}.bam.bai",
        bed=config.get("reference", {}).get("exon_bed", ""),
    output:
        bed=temp("qc/mosdepth_bed_coding/{sample}_{type}.regions.bed.gz"),
        bed_csi=temp("qc/mosdepth_bed_coding/{sample}_{type}.regions.bed.gz.csi"),
        coverage=temp("qc/mosdepth_bed_coding/{sample}_{type}.per-base.bed.gz"),
        coverage_csi=temp("qc/mosdepth_bed_coding/{sample}_{type}.per-base.bed.gz.csi"),
        thresholds=temp("qc/mosdepth_bed_coding/{sample}_{type}.thresholds.bed.gz"),
        glob=temp("qc/mosdepth_bed_coding/{sample}_{type}.mosdepth.global.dist.txt"),
        region=temp("qc/mosdepth_bed_coding/{sample}_{type}.mosdepth.region.dist.txt"),
        summary=temp("qc/mosdepth_bed_coding/{sample}_{type}.mosdepth.summary.txt"),
    params:
        thresholds=config["mosdepth_bed"]["thresholds"],
    log:
        "qc/mosdepth_bed_coding/{sample}_{type}.mosdepth.summary.txt.log",
    benchmark:
        repeat(
            "qc/mosdepth_bed_coding/{sample}_{type}.mosdepth.summary.txt.benchmark.tsv",
            config.get("mosdepth_bed", {}).get("benchmark_repeats", 1),
        )


use rule multiqc from qc as qc_multiqc with:
    input:
        files=lambda wildcards: set(
            [
                file.format(sample=sample, type=u.type, lane=u.lane, flowcell=u.flowcell, barcode=u.barcode, read=read, ext=ext)
                for file in config["multiqc"]["reports"][wildcards.report]["qc_files"]
                for sample in get_samples(samples)
                for u in units.loc[sample].dropna().itertuples()
                if u.type in config["multiqc"]["reports"][wildcards.report]["included_unit_types"]
                for read in ["fastq1", "fastq2"]
                for ext in config.get("picard_collect_multiple_metrics", {}).get("output_ext", [""])
            ]
        ),
        config=lambda wildcards: config["multiqc"]["reports"][wildcards.report]["config"],
        sample_order="qc/multiqc/sample_order.tsv",
        sample_replacement="qc/multiqc/sample_replacement.tsv",
    params:
        extra=lambda wildcards, input: "--replace-names "
        + input.sample_replacement
        + " --sample-names "
        + input.sample_order
        + " -c "
        + input.config,
